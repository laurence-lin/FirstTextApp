{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea2743e-1d44-4982-948d-514b17f87aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done import packages..\n"
     ]
    }
   ],
   "source": [
    "from django.utils import timezone\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "#summarizer = pipeline(\"summarization\")\n",
    "\n",
    "\n",
    "print(\"Done import packages..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02bebb88-0041-4142-81f5-fd01bef42574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done define text.\n"
     ]
    }
   ],
   "source": [
    "text_2 = '''\n",
    "Last week India further slashed import duties on motorcycles, cutting tariffs on heavyweight bikes with engines above 1,600cc from 50% to 30% and smaller ones from 50% to 40%.\n",
    "\n",
    "A pre-emptive move designed to further smoothen the entry of Harley Davidsons into India - and, Delhi hopes, ward off any threat of tariffs. US motorcycle exports to India were worth $3m last year.\n",
    "\n",
    "Donald Trump has marked his return to the White House by brandishing trade measures against America's neighbours and allies as well as its big rival China.\n",
    "\n",
    "India hopes it is ahead of the game - but will its tariff cuts satisfy Trump, or is trade action still on the table?\n",
    "\n",
    "\"Canada and Mexico are literally two arms of the US. If he has acted against them, he could easily act against India too,\" says Ajay Srivastava, founder of the Delhi-based think tank Global Trade Research Institute (GTRI).\n",
    "\n",
    "In their phone conversation late last month, the US president pressed Prime Minister Narendra Modi to buy more US arms and for there to be a fairer trade balance, keeping the pressure on.\n",
    "\n",
    "And during his first term, Trump fixated on India's steep tariffs. He repeatedly slammed the then 100% duty on Harleys as \"unacceptable\", making it a rallying point in his crusade against what he saw as unfair trade practices.\n",
    "\n",
    "In the past he repeatedly branded India a \"tariff king\" and a \"big abuser\" of trade ties.\n",
    "\n",
    "India enjoys a trade surplus with the US, its top trading partner. Bilateral trade crossed $190bn (£150bn) in 2023. Merchandise exports to US have surged 40% to $123bn since 2018, while services trade grew 22% to reach $66bn. Meanwhile, US exports to India stood at $70bn.\n",
    "\n",
    "But beyond bikes, India has zeroed out import taxes on satellite ground installations, benefiting US exporters who supplied $92m worth in 2023.\n",
    "\n",
    "Tariffs on synthetic flavouring essences dropped from 100% to 20% ($21m in US exports last year), while duties on fish hydrolysate for aquatic feed fell from 15% to 5% ($35m in US exports in 2024). India also scrapped tariffs on select waste and scrap items, a category where US exports amounted to $2.5bn last year.\n",
    "\n",
    "Top US exports to India in 2023 included crude oil and petroleum products ($14bn), LNG, coal, medical devices, scientific instruments, scrap metals, turbojets, computers and almonds.\n",
    "\n",
    "\"While Trump has criticised India's tariff policies, the latest reductions signal a policy shift that could enhance US exports across various sectors,\" says Mr Srivastava.\n",
    "\n",
    "\"With key tariff cuts on technology, automobiles, industrial and waste imports, India appears to be taking steps towards facilitating trade even as the global trade environment remains tense.\"\n",
    "'''\n",
    "\n",
    "print(\"Done define text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafe5d1c-a800-44c6-8458-fd5f06bd225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done define functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def translate_long_text(text, chunk_size=500, model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024', device = 'cpu', translation_order = 'translate to zh: '):  # Adjust chunk_size as needed\n",
    "    \"\"\"Translates long text to Chinese by chunking.\"\"\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name) # load model\n",
    "    model.to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name) # load model tokenizer\n",
    "\n",
    "    src_text = translation_order + text\n",
    "    print(\"src_text: \", src_text)\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\").input_ids\n",
    "    print(\"input ids: \", input_ids)\n",
    "    max_len = tokenizer.model_max_length - 2 # Account for start/end tokens with the tokenizer's max length\n",
    "\n",
    "    translated_chunks = []\n",
    "    for i in range(0, input_ids.shape[1], max_len):\n",
    "        print(\"Working on chunk \", (i // max_len) + 1)\n",
    "        chunk_input_ids = input_ids[:, i:min(i + max_len, input_ids.shape[1])]\n",
    "        generated_tokens = model.generate(chunk_input_ids.to(device))\n",
    "\n",
    "        translated_chunk = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        translated_chunks.append(translated_chunk)\n",
    "\n",
    "    return \"\".join(translated_chunks)  # Combine translated chunks\n",
    "\n",
    "\n",
    "print(\"Done define functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf711a-c3c3-4f17-8c8e-20ba5ae28984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator\n",
    "#text = \"How old are you?\"\n",
    "\n",
    "print(\"Length of input text for translation: \", len(text))\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "result = translate_long_text(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56bdfdd-6190-4cd2-9c45-368d37bd1240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len of current model:  1022\n",
      "src_text:  translate to zh: \n",
      "Last week India further slashed import duties on motorcycles, cutting tariffs on heavyweight bikes with engines above 1,600cc from 50% to 30% and smaller ones from 50% to 40%.\n",
      "\n",
      "A pre-emptive move designed to further smoothen the entry of Harley Davidsons into India - and, Delhi hopes, ward off any threat of tariffs. US motorcycle exports to India were worth $3m last year.\n",
      "\n",
      "Donald Trump has marked his return to the White House by brandishing trade measures against America's neighbours and allies as well as its big rival China.\n",
      "\n",
      "India hopes it is ahead of the game - but will its tariff cuts satisfy Trump, or is trade action still on the table?\n",
      "\n",
      "\"Canada and Mexico are literally two arms of the US. If he has acted against them, he could easily act against India too,\" says Ajay Srivastava, founder of the Delhi-based think tank Global Trade Research Institute (GTRI).\n",
      "\n",
      "In their phone conversation late last month, the US president pressed Prime Minister Narendra Modi to buy more US arms and for there to be a fairer trade balance, keeping the pressure on.\n",
      "\n",
      "And during his first term, Trump fixated on India's steep tariffs. He repeatedly slammed the then 100% duty on Harleys as \"unacceptable\", making it a rallying point in his crusade against what he saw as unfair trade practices.\n",
      "\n",
      "In the past he repeatedly branded India a \"tariff king\" and a \"big abuser\" of trade ties.\n",
      "\n",
      "India enjoys a trade surplus with the US, its top trading partner. Bilateral trade crossed $190bn (£150bn) in 2023. Merchandise exports to US have surged 40% to $123bn since 2018, while services trade grew 22% to reach $66bn. Meanwhile, US exports to India stood at $70bn.\n",
      "\n",
      "But beyond bikes, India has zeroed out import taxes on satellite ground installations, benefiting US exporters who supplied $92m worth in 2023.\n",
      "\n",
      "Tariffs on synthetic flavouring essences dropped from 100% to 20% ($21m in US exports last year), while duties on fish hydrolysate for aquatic feed fell from 15% to 5% ($35m in US exports in 2024). India also scrapped tariffs on select waste and scrap items, a category where US exports amounted to $2.5bn last year.\n",
      "\n",
      "Top US exports to India in 2023 included crude oil and petroleum products ($14bn), LNG, coal, medical devices, scientific instruments, scrap metals, turbojets, computers and almonds.\n",
      "\n",
      "\"While Trump has criticised India's tariff policies, the latest reductions signal a policy shift that could enhance US exports across various sectors,\" says Mr Srivastava.\n",
      "\n",
      "\"With key tariff cuts on technology, automobiles, industrial and waste imports, India appears to be taking steps towards facilitating trade even as the global trade environment remains tense.\"\n",
      "\n",
      "input ids:  tensor([[21809,    19,     4,  3631,    31,  3923,  2295,  2185,  1232,     4,\n",
      "            13, 12853,    71,  9626, 16343,    45, 36234,    13,     3, 11138,\n",
      "         34118,    45,  5704, 16434, 23367,    13,    46, 11709,  1308,     4,\n",
      "             8,     3,    22,     6,     6,  7898,    60,     4,    18,     6,\n",
      "           120,    19,     4,    17,     6,   120,    12,  6867,  6691,    60,\n",
      "             4,    18,     6,   120,    19,     4,    20,     6,  3798,    95,\n",
      "           843,     9, 25455,   749,  3784,  1866,    19,  1232, 13026,   383,\n",
      "             7,  8719,    11,  4011,  2497,  5022,  1076,    13,   202,  2185,\n",
      "            35,    12,     3, 29641, 20808,     3,     4,  5345,  1146,   267,\n",
      "          8388,    11, 34118,     5,   881, 36234, 12523,    19,  2185,   125,\n",
      "          4495,   175,    17,   168,   667,   323,     5, 25450,  7195,   102,\n",
      "         12296,   121,  2815,    19,     7,  5942,  3134,    54,  4803, 30366,\n",
      "          1418,  2084,   549,  1610,    38,    13, 50314,    12, 21884,    53,\n",
      "           300,    53,   118,  2571, 19245,   581,     5,  2185, 20808,    73,\n",
      "            36, 12006,    11,     7,  1990,    35,   142,   104,   118, 36849,\n",
      "         28939, 21298,  7195,     3,    67,    36,  1418,  1886,   802,    45,\n",
      "             7,  3655,    99,    39, 34831,    12,  5890,    64, 23098,   225,\n",
      "          7052,    11,     7,   881,     5,   543,   131,   102, 30829,   549,\n",
      "           263,     3,   131,   449,  4653,  4063,   549,  2185,  1486,   821,\n",
      "          1430,    95,  1380,   128, 11438, 12024,    93, 10178,     3, 10433,\n",
      "            11,     7, 29641,     9,  1077,  1205, 10950,  3458,  3588,  2612,\n",
      "          1608,    25,   372, 27690,    84,    97,   116,  3047, 12111,  2967,\n",
      "           667,  2378,     3,     7,   881,  3924,     4, 22112,  6785,  1736,\n",
      "         15500, 48749,  1749,  1208,    19,  4260,   139,   881,  7052,    12,\n",
      "            37,   242,    19,    63,    27,  8169,   129,  1418,  3999,     3,\n",
      "         13944,     7,  2619,    45,     5,   478,   466,   121,   205,  2712,\n",
      "             3,  7195, 15685,  1090,    45,  2185,    38,    13, 34238, 34118,\n",
      "             5,   347, 22912,     4,    13,  7770,  6468,     7,   473,     4,\n",
      "             8,     6,     6,   120, 10283,    45,  4011,  2497,    13,    53,\n",
      "            39,   736, 45848,   422,   376,  1800,    73,    27, 33557,    61,\n",
      "          1321,    21,   121,     4,  6422,    13,  6222,   549,   387,   131,\n",
      "          3246,    53, 45209,  1418,  5623,     5,    97,     7,  1425,   131,\n",
      "         22912,     4, 40780,  2185,    27,    39,   761, 28446,  6907,    66,\n",
      "            12,    27,    39, 28342,  8582,   238,    66,    11,  1418, 22575,\n",
      "             5,  2185,  4976,    13,    27,  1418, 28999,    46,     7,   881,\n",
      "             3,   118,  1766,  7171,  4884,     5,  4740, 20358,  1418, 27482,\n",
      "           175,     8,    15,     6,  7018, 36530,     8,    18,     6,  7018,\n",
      "            28,    21,     4,    10,     6,    10,    17,     5,     4, 37949,\n",
      "          7238,  2998, 12523,    19,   881,    91, 28957,    88,     4,    20,\n",
      "             6,   120,    19,   175,     8,    10,    17,  7018,   697,     4,\n",
      "            10,     6,     8,    24,     3,   588,   604,  1418, 10099,     4,\n",
      "            10,    10,   120,    19,  3776,   175,    22,    22,  7018,     5,\n",
      "         15165,     3,   881, 12523,    19,  2185, 11965,    68,   175,    26,\n",
      "             6,  7018,     5,   610,  4883, 23367,    13,     3,  2185,   102,\n",
      "         12625,    71,   217,  9626, 13797,    45,  8341,  3097, 28231,     3,\n",
      "          4476,    61,   881,  5289,   349,   148, 19328,   175,    15,    10,\n",
      "           168,  4495,    21,     4,    10,     6,    10,    17,     5,  1973,\n",
      "         28446,    13,    45, 22700, 53588,    61, 21428,    13, 15975,    60,\n",
      "             4,     8,     6,     6,   120,    19,     4,    10,     6,   120,\n",
      "          1478,    10,     8,   168,    21,   881, 12523,   667,   323,    74,\n",
      "           588, 16343,    45,  3958,  9099,   114,    13,   700,    37, 51295,\n",
      "         10131,  7565,    60,     4,     8,    18,   120,    19,     4,    18,\n",
      "           120,  1478,    17,    18,   168,    21,   881, 12523,    21,     4,\n",
      "            10,     6,    10,    20,    84,  2185,   147, 30349,  3826, 34118,\n",
      "            45,  7806,  5429,    12, 30349,  5038,     3,    27,  7062,   373,\n",
      "           881, 12523,  6759,    19,   175,    10,     5,    18,  7018,   667,\n",
      "           323,     5, 10039,   881, 12523,    19,  2185,    21,     4,    10,\n",
      "             6,    10,    17,  2055, 24340,  1387,    12, 32485,   696,  1478,\n",
      "             8,    20,  7018,    74,     4, 35076,     3,  9570,     3,  2059,\n",
      "          3037,     3,  2705,  7568,     3, 30349, 19230,     3, 38629, 25115,\n",
      "            13,     3, 13386,    12,  1374, 16235,    13,     5,    39, 42440,\n",
      "          7195,   102, 24083,  3403,  2185,    38,    13, 36849,  4328,     3,\n",
      "             7,  4374,  4689,    13,  6397,    27,  1534,  7718,    42,   449,\n",
      "          8491,   881, 12523,  1902,  1164, 10205,   821,  1430,   551, 11438,\n",
      "         12024,    93, 10178,     5,    39, 17899,  1855, 36849, 28939,    45,\n",
      "           915,     3, 25716,    13,     3,  2734,    12,  5429, 20156,     3,\n",
      "          2185,  7591,    19,    63,  2304,  6605,  3216, 36676,  1418,   475,\n",
      "            53,     7,  1350,  1418,  2120,  4486,     4, 44802,   434,     1]])\n",
      "Working on chunk  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tokenize_text(text, chunk_size=500, model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024', device = 'cpu', \n",
    "                  translation_order = 'translate to zh: '):  # Adjust chunk_size as needed\n",
    "    \"\"\"Translates long text to Chinese by chunking.\"\"\"\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name) # load model tokenizer\n",
    "    max_len = tokenizer.model_max_length - 2 # Account for start/end tokens with the tokenizer's max length\n",
    "    print(\"Max len of current model: \", max_len)\n",
    "    \n",
    "    src_text = translation_order + text\n",
    "    print(\"src_text: \", src_text)\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\").input_ids\n",
    "    print(\"input ids: \", input_ids)\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for i in range(0, input_ids.shape[1], max_len):\n",
    "        print(\"Working on chunk \", (i // max_len) + 1)\n",
    "        \n",
    "        #chunk_input_ids = input_ids[:, i:min(i + max_len, input_ids.shape[1])]\n",
    "        #generated_tokens = model.generate(chunk_input_ids.to(device))\n",
    "\n",
    "\n",
    "    return \"\".join(translated_chunks)  # Combine translated chunks\n",
    "\n",
    "\n",
    "\n",
    "result = tokenize_text(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8914046-b4b6-4eef-896c-e5e7b83c98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input: {'input_ids': tensor([[ 2506,   471,  1547,   856,     3,     7,   521,  7420,  4830,  9353,\n",
      "            30, 11718,     7,     6,  3753, 20116,     7,    30,  2437,  9378,\n",
      "         13490,    28,  7277,   756,  1914,  6007,    75,    75,    45,  5743,\n",
      "            12, 10738,    11,  2755,  2102,    45,  5743,    12, 13152,     5,\n",
      "            71,   554,    18,  9045,   757,   888,   876,    12,   856,  3050,\n",
      "            35,     8,  1764,    13, 26085,     3, 23268,     7,   139,  1547,\n",
      "             3,    18,    11,     6, 10619,  7511,     6,     3,  2239,   326,\n",
      "           136,  5888,    13, 20116,     7,     5,   837, 11718,  4202,     7,\n",
      "            12,  1547,   130,  1494,  5583,    51,   336,   215,     5,  7459,\n",
      "          2523,    65,  7027,   112,  1205,    12,     8,  1945,  1384,    57,\n",
      "          1056,  1273,    53,  1668,  3629,   581,  1371,    31,     7, 14245,\n",
      "             7,    11,    66,   725,    38,   168,    38,   165,   600,  8374,\n",
      "          1473,     5,  1547,  7511,    34,    19,  2177,    13,     8,   467,\n",
      "             3,    18,    68,    56,   165, 20116,  8620, 11132,  2523,     6,\n",
      "            42,    19,  1668,  1041,   341,    30,     8,   953,    58,    96,\n",
      "         28811,    11,  4726,    33,  6672,   192,  6026,    13,     8,   837,\n",
      "             5,   156,     3,    88,    65,     3,  9925,   581,   135,     6,\n",
      "             3,    88,   228,  1153,  1810,   581,  1547,   396,   976,   845,\n",
      "            71,  1191,    63,  8642,  9856,    17,  8644,     6,  7174,    13,\n",
      "             8, 10619,    18,   390,   317,  5040,  3699,  6550,  2200,  2548,\n",
      "            41,   517, 16840,   137,    86,    70,   951,  3634,  1480,   336,\n",
      "           847,     6,     8,   837,  2753,     3,  8918,  5923,  3271, 13346,\n",
      "         12524,  5073,    23,    12,   805,    72,   837,  6026,    11,    21,\n",
      "           132,    12,    36,     3,     9,  1143,    52,  1668,  2109,     6,\n",
      "          2627,     8,  1666,    30,     5,   275,   383,   112,   166,  1657,\n",
      "             6,  2523,  2210,   920,    30,  1547,    31,     7, 10856, 20116,\n",
      "             7,     5,   216, 16049,     3,     7,    40,   265,  2726,     8,\n",
      "           258,  2349,  5461,    30, 26085,     7,    38,    96,   202, 18693,\n",
      "           179,  1686,   492,    34,     3,     9, 13980,    53,   500,    16,\n",
      "           112,  8396,     7,     9,   221,   581,   125,     3,    88,  1509,\n",
      "            38, 18216,  1668,  2869,     5,    86,     8,   657,     3,    88,\n",
      "         16049,     3, 16108,  1547,     3,     9,    96,  2046,  5982,     3,\n",
      "          1765,   121,    11,     3,     9,    96, 12911,  5384,    52,   121,\n",
      "            13,  1668,     3,  3010,     5,  1547,   777,     7,     3,     9,\n",
      "          1668, 20524,    28,     8,   837,     6,   165,   420,  3415,  2397,\n",
      "             5,  2106, 12088,  1668, 14602,  1970,  2394,   115,    29,    41,\n",
      "         19853, 12278,   115,    29,    61,    16,   460,  2773,     5,     3,\n",
      "         18180,  2894,   159,    15,  4202,     7,    12,   837,    43, 18787,\n",
      "            26, 13152,    12,  1970,  2773,   115,    29,   437,  4323,   298,\n",
      "           364,  1668,     3,  4774,   204,  5406,    12,  1535,  1514,  3539,\n",
      "           115,    29,     5,  7940,     6,   837,  4202,     7,    12,  1547,\n",
      "          8190,    44,  1514,  2518,   115,    29,     5,   299,  1909, 13490,\n",
      "             6,  1547,    65,  5733,    15,    26,    91,  4830,  5161,    30,\n",
      "          7605,  1591, 12284,     6,  1656,    53,   837,  4202,   277,   113,\n",
      "          8794, 16616,   357,    51,  1494,    16,   460,  2773,     5, 18826,\n",
      "            89,     7,    30, 13699,  9499,    53, 10848,     7,  6292,    45,\n",
      "          2349,    12,  7580,  8785,  2658,    51,    16,   837,  4202,     7,\n",
      "           336,   215,   201,   298,  9353,    30,  2495,  7668,   120,     7,\n",
      "           342,    21, 28342,  3305,  4728,    45, 13914,    12,     3,  2712,\n",
      "          8785,  2469,    51,    16,   837,  4202,     7,    16,   460,  2266,\n",
      "           137,  1547,    92,  7346,  3138, 20116,     7,    30,  1738,  2670,\n",
      "            11,  7346,  1173,     6,     3,     9,  3295,   213,   837,  4202,\n",
      "             7,     3,     9, 22983,    12,  1514, 15967,   115,    29,   336,\n",
      "           215,     5,  2224,   837,  4202,     7,    12,  1547,    16,   460,\n",
      "          2773,  1285, 19058,  1043,    11, 27957,   494,  8785,  2534,   115,\n",
      "            29,   201, 28323,     6,  8416,     6,  1035,  1904,     6,  4290,\n",
      "          7778,     6,  7346,  1946,     7,     6, 20353,  9245,     7,     6,\n",
      "          7827,    11, 15764,     7,     5,    96, 27674,  2523,    65,  6800,\n",
      "          3375,  1547,    31,     7, 20116,  3101,     6,     8,  1251,  4709,\n",
      "             7,  3240,     3,     9,  1291,  4108,    24,   228,  3391,   837,\n",
      "          4202,     7,   640,   796,  8981,   976,   845,  1363,  8642,  9856,\n",
      "            17,  8644,     5,    96, 15013,   843, 20116,  8620,    30,   748,\n",
      "             6,  8395,     7,     6,  2913,    11,  2670,  4830,     7,     6,\n",
      "          1547,  3475,    12,    36,   838,  2245,  1587,     3, 24962,  1668,\n",
      "           237,    38,     8,  1252,  1668,  1164,  3048,     3,    17,  5167,\n",
      "           535,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n",
      "Input IDs: tensor([[ 2506,   471,  1547,   856,     3,     7,   521,  7420,  4830,  9353,\n",
      "            30, 11718,     7,     6,  3753, 20116,     7,    30,  2437,  9378,\n",
      "         13490,    28,  7277,   756,  1914,  6007,    75,    75,    45,  5743,\n",
      "            12, 10738,    11,  2755,  2102,    45,  5743,    12, 13152,     5,\n",
      "            71,   554,    18,  9045,   757,   888,   876,    12,   856,  3050,\n",
      "            35,     8,  1764,    13, 26085,     3, 23268,     7,   139,  1547,\n",
      "             3,    18,    11,     6, 10619,  7511,     6,     3,  2239,   326,\n",
      "           136,  5888,    13, 20116,     7,     5,   837, 11718,  4202,     7,\n",
      "            12,  1547,   130,  1494,  5583,    51,   336,   215,     5,  7459,\n",
      "          2523,    65,  7027,   112,  1205,    12,     8,  1945,  1384,    57,\n",
      "          1056,  1273,    53,  1668,  3629,   581,  1371,    31,     7, 14245,\n",
      "             7,    11,    66,   725,    38,   168,    38,   165,   600,  8374,\n",
      "          1473,     5,  1547,  7511,    34,    19,  2177,    13,     8,   467,\n",
      "             3,    18,    68,    56,   165, 20116,  8620, 11132,  2523,     6,\n",
      "            42,    19,  1668,  1041,   341,    30,     8,   953,    58,    96,\n",
      "         28811,    11,  4726,    33,  6672,   192,  6026,    13,     8,   837,\n",
      "             5,   156,     3,    88,    65,     3,  9925,   581,   135,     6,\n",
      "             3,    88,   228,  1153,  1810,   581,  1547,   396,   976,   845,\n",
      "            71,  1191,    63,  8642,  9856,    17,  8644,     6,  7174,    13,\n",
      "             8, 10619,    18,   390,   317,  5040,  3699,  6550,  2200,  2548,\n",
      "            41,   517, 16840,   137,    86,    70,   951,  3634,  1480,   336,\n",
      "           847,     6,     8,   837,  2753,     3,  8918,  5923,  3271, 13346,\n",
      "         12524,  5073,    23,    12,   805,    72,   837,  6026,    11,    21,\n",
      "           132,    12,    36,     3,     9,  1143,    52,  1668,  2109,     6,\n",
      "          2627,     8,  1666,    30,     5,   275,   383,   112,   166,  1657,\n",
      "             6,  2523,  2210,   920,    30,  1547,    31,     7, 10856, 20116,\n",
      "             7,     5,   216, 16049,     3,     7,    40,   265,  2726,     8,\n",
      "           258,  2349,  5461,    30, 26085,     7,    38,    96,   202, 18693,\n",
      "           179,  1686,   492,    34,     3,     9, 13980,    53,   500,    16,\n",
      "           112,  8396,     7,     9,   221,   581,   125,     3,    88,  1509,\n",
      "            38, 18216,  1668,  2869,     5,    86,     8,   657,     3,    88,\n",
      "         16049,     3, 16108,  1547,     3,     9,    96,  2046,  5982,     3,\n",
      "          1765,   121,    11,     3,     9,    96, 12911,  5384,    52,   121,\n",
      "            13,  1668,     3,  3010,     5,  1547,   777,     7,     3,     9,\n",
      "          1668, 20524,    28,     8,   837,     6,   165,   420,  3415,  2397,\n",
      "             5,  2106, 12088,  1668, 14602,  1970,  2394,   115,    29,    41,\n",
      "         19853, 12278,   115,    29,    61,    16,   460,  2773,     5,     3,\n",
      "         18180,  2894,   159,    15,  4202,     7,    12,   837,    43, 18787,\n",
      "            26, 13152,    12,  1970,  2773,   115,    29,   437,  4323,   298,\n",
      "           364,  1668,     3,  4774,   204,  5406,    12,  1535,  1514,  3539,\n",
      "           115,    29,     5,  7940,     6,   837,  4202,     7,    12,  1547,\n",
      "          8190,    44,  1514,  2518,   115,    29,     5,   299,  1909, 13490,\n",
      "             6,  1547,    65,  5733,    15,    26,    91,  4830,  5161,    30,\n",
      "          7605,  1591, 12284,     6,  1656,    53,   837,  4202,   277,   113,\n",
      "          8794, 16616,   357,    51,  1494,    16,   460,  2773,     5, 18826,\n",
      "            89,     7,    30, 13699,  9499,    53, 10848,     7,  6292,    45,\n",
      "          2349,    12,  7580,  8785,  2658,    51,    16,   837,  4202,     7,\n",
      "           336,   215,   201,   298,  9353,    30,  2495,  7668,   120,     7,\n",
      "           342,    21, 28342,  3305,  4728,    45, 13914,    12,     3,  2712,\n",
      "          8785,  2469,    51,    16,   837,  4202,     7,    16,   460,  2266,\n",
      "           137,  1547,    92,  7346,  3138, 20116,     7,    30,  1738,  2670,\n",
      "            11,  7346,  1173,     6,     3,     9,  3295,   213,   837,  4202,\n",
      "             7,     3,     9, 22983,    12,  1514, 15967,   115,    29,   336,\n",
      "           215,     5,  2224,   837,  4202,     7,    12,  1547,    16,   460,\n",
      "          2773,  1285, 19058,  1043,    11, 27957,   494,  8785,  2534,   115,\n",
      "            29,   201, 28323,     6,  8416,     6,  1035,  1904,     6,  4290,\n",
      "          7778,     6,  7346,  1946,     7,     6, 20353,  9245,     7,     6,\n",
      "          7827,    11, 15764,     7,     5,    96, 27674,  2523,    65,  6800,\n",
      "          3375,  1547,    31,     7, 20116,  3101,     6,     8,  1251,  4709,\n",
      "             7,  3240,     3,     9,  1291,  4108,    24,   228,  3391,   837,\n",
      "          4202,     7,   640,   796,  8981,   976,   845,  1363,  8642,  9856,\n",
      "            17,  8644,     5,    96, 15013,   843, 20116,  8620,    30,   748,\n",
      "             6,  8395,     7,     6,  2913,    11,  2670,  4830,     7,     6,\n",
      "          1547,  3475,    12,    36,   838,  2245,  1587,     3, 24962,  1668,\n",
      "           237,    38,     8,  1252,  1668,  1164,  3048,     3,    17,  5167,\n",
      "           535,     1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Choose a Model and Corresponding Tokenizer:\n",
    "model_name = \"t5-small\"\n",
    "#model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "\n",
    "# Initialize the Tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Example Text to Tokenize:\n",
    "text = \"This is an example sentence to be tokenized. Let's see how it works!\"\n",
    "\n",
    "# 3. Tokenization:\n",
    "\n",
    "# Tokenize the text:\n",
    "encoded_input = tokenizer(text_2, return_tensors=\"pt\") # pt for PyTorch tensors\n",
    "\n",
    "# Print the encoded input (a dictionary):\n",
    "print(\"Encoded Input:\", encoded_input)\n",
    "\n",
    "# Access the input IDs (the numerical representation of the tokens):\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "023f193a-65c1-49f3-84b6-3d1f3ae27b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is an example sentence to be tokenized. Let's see how it works!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "158540ec-ca5d-4853-8be2-61a671c439f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_char = len(text_2.split(' '))\n",
    "num_of_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac1dc86-e186-488c-aeaa-550211b82b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37215f41-5033-45db-8da4-727465490f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])\n",
      "Decoded Text: Last week India further slashed import duties on motorcycles, cutting tariffs on heavyweight bikes with engines above 1,600cc from 50% to 30% and smaller ones from 50% to 40%. A pre-emptive move designed to further smoothen the entry of Harley Davidsons into India - and, Delhi hopes, ward off any threat of tariffs. US motorcycle exports to India were worth $3m last year. Donald Trump has marked his return to the White House by brandishing trade measures against America's neighbours and allies as well as its big rival China. India hopes it is ahead of the game - but will its tariff cuts satisfy Trump, or is trade action still on the table? \"Canada and Mexico are literally two arms of the US. If he has acted against them, he could easily act against India too,\" says Ajay Srivastava, founder of the Delhi-based think tank Global Trade Research Institute (GTRI). In their phone conversation late last month, the US president pressed Prime Minister Narendra Modi to buy more US arms and for there to be a fairer trade balance, keeping the pressure on. And during his first term, Trump fixated on India's steep tariffs. He repeatedly slammed the then 100% duty on Harleys as \"unacceptable\", making it a rallying point in his crusade against what he saw as unfair trade practices. In the past he repeatedly branded India a \"tariff king\" and a \"big abuser\" of trade ties. India enjoys a trade surplus with the US, its top trading partner. Bilateral trade crossed $190bn (£150bn) in 2023. Merchandise exports to US have surged 40% to $123bn since 2018, while services trade grew 22% to reach $66bn. Meanwhile, US exports to India stood at $70bn. But beyond bikes, India has zeroed out import taxes on satellite ground installations, benefiting US exporters who supplied $92m worth in 2023. Tariffs on synthetic flavouring essences dropped from 100% to 20% ($21m in US exports last year), while duties on fish hydrolysate for aquatic feed fell from 15% to 5% ($35m in US exports in 2024). India also scrapped tariffs on select waste and scrap items, a category where US exports amounted to $2.5bn last year. Top US exports to India in 2023 included crude oil and petroleum products ($14bn), LNG, coal, medical devices, scientific instruments, scrap metals, turbojets, computers and almonds. \"While Trump has criticised India's tariff policies, the latest reductions signal a policy shift that could enhance US exports across various sectors,\" says Mr Srivastava. \"With key tariff cuts on technology, automobiles, industrial and waste imports, India appears to be taking steps towards facilitating trade even as the global trade environment remains tense.\"\n"
     ]
    }
   ],
   "source": [
    "# Access the attention mask (indicates which tokens should be attended to):\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "# 4. Decoding (Converting back to text):\n",
    "\n",
    "# Decode the input IDs back to text:\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True) # [0] for the first sequence\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fe74853-917a-4ae6-8b0d-0c986d320bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁Last', '▁week', '▁India', '▁further', '▁', 's', 'la', 'shed', '▁import', '▁duties', '▁on', '▁motorcycle', 's', ',', '▁cutting', '▁tariff', 's', '▁on', '▁heavy', 'weight', '▁bikes', '▁with', '▁engines', '▁above', '▁1,', '600', 'c', 'c', '▁from', '▁50%', '▁to', '▁30%', '▁and', '▁smaller', '▁ones', '▁from', '▁50%', '▁to', '▁40%', '.', '▁A', '▁pre', '-', 'empt', 'ive', '▁move', '▁designed', '▁to', '▁further', '▁smooth', 'en', '▁the', '▁entry', '▁of', '▁Harley', '▁', 'Davidson', 's', '▁into', '▁India', '▁', '-', '▁and', ',', '▁Delhi', '▁hopes', ',', '▁', 'ward', '▁off', '▁any', '▁threat', '▁of', '▁tariff', 's', '.', '▁US', '▁motorcycle', '▁export', 's', '▁to', '▁India', '▁were', '▁worth', '▁$3', 'm', '▁last', '▁year', '.', '▁Donald', '▁Trump', '▁has', '▁marked', '▁his', '▁return', '▁to', '▁the', '▁White', '▁House', '▁by', '▁brand', 'ish', 'ing', '▁trade', '▁measures', '▁against', '▁America', \"'\", 's', '▁neighbour', 's', '▁and', '▁all', 'ies', '▁as', '▁well', '▁as', '▁its', '▁big', '▁rival', '▁China', '.', '▁India', '▁hopes', '▁it', '▁is', '▁ahead', '▁of', '▁the', '▁game', '▁', '-', '▁but', '▁will', '▁its', '▁tariff', '▁cuts', '▁satisfy', '▁Trump', ',', '▁or', '▁is', '▁trade', '▁action', '▁still', '▁on', '▁the', '▁table', '?', '▁\"', 'Canada', '▁and', '▁Mexico', '▁are', '▁literally', '▁two', '▁arms', '▁of', '▁the', '▁US', '.', '▁If', '▁', 'he', '▁has', '▁', 'acted', '▁against', '▁them', ',', '▁', 'he', '▁could', '▁easily', '▁act', '▁against', '▁India', '▁too', ',\"', '▁says', '▁A', 'ja', 'y', '▁Sri', 'vas', 't', 'ava', ',', '▁founder', '▁of', '▁the', '▁Delhi', '-', 'based', '▁think', '▁tank', '▁Global', '▁Trade', '▁Research', '▁Institute', '▁(', 'G', 'TRI', ').', '▁In', '▁their', '▁phone', '▁conversation', '▁late', '▁last', '▁month', ',', '▁the', '▁US', '▁president', '▁', 'pressed', '▁Prime', '▁Minister', '▁Nar', 'endra', '▁Mod', 'i', '▁to', '▁buy', '▁more', '▁US', '▁arms', '▁and', '▁for', '▁there', '▁to', '▁be', '▁', 'a', '▁faire', 'r', '▁trade', '▁balance', ',', '▁keeping', '▁the', '▁pressure', '▁on', '.', '▁And', '▁during', '▁his', '▁first', '▁term', ',', '▁Trump', '▁fix', 'ated', '▁on', '▁India', \"'\", 's', '▁steep', '▁tariff', 's', '.', '▁He', '▁repeatedly', '▁', 's', 'l', 'am', 'med', '▁the', '▁then', '▁100%', '▁duty', '▁on', '▁Harley', 's', '▁as', '▁\"', 'un', 'accept', 'able', '\",', '▁making', '▁it', '▁', 'a', '▁rally', 'ing', '▁point', '▁in', '▁his', '▁cru', 's', 'a', 'de', '▁against', '▁what', '▁', 'he', '▁saw', '▁as', '▁unfair', '▁trade', '▁practices', '.', '▁In', '▁the', '▁past', '▁', 'he', '▁repeatedly', '▁', 'branded', '▁India', '▁', 'a', '▁\"', 'tar', 'iff', '▁', 'king', '\"', '▁and', '▁', 'a', '▁\"', 'big', '▁abuse', 'r', '\"', '▁of', '▁trade', '▁', 'ties', '.', '▁India', '▁enjoy', 's', '▁', 'a', '▁trade', '▁surplus', '▁with', '▁the', '▁US', ',', '▁its', '▁top', '▁trading', '▁partner', '.', '▁Bi', 'lateral', '▁trade', '▁crossed', '▁$1', '90', 'b', 'n', '▁(', '£', '150', 'b', 'n', ')', '▁in', '▁20', '23', '.', '▁', 'Merc', 'hand', 'is', 'e', '▁export', 's', '▁to', '▁US', '▁have', '▁surge', 'd', '▁40%', '▁to', '▁$1', '23', 'b', 'n', '▁since', '▁2018,', '▁while', '▁services', '▁trade', '▁', 'grew', '▁2', '2%', '▁to', '▁reach', '▁$', '66', 'b', 'n', '.', '▁Meanwhile', ',', '▁US', '▁export', 's', '▁to', '▁India', '▁stood', '▁at', '▁$', '70', 'b', 'n', '.', '▁But', '▁beyond', '▁bikes', ',', '▁India', '▁has', '▁zero', 'e', 'd', '▁out', '▁import', '▁taxes', '▁on', '▁satellite', '▁ground', '▁installations', ',', '▁benefit', 'ing', '▁US', '▁export', 'ers', '▁who', '▁supplied', '▁$9', '2', 'm', '▁worth', '▁in', '▁20', '23', '.', '▁Tarif', 'f', 's', '▁on', '▁synthetic', '▁flavour', 'ing', '▁essence', 's', '▁dropped', '▁from', '▁100%', '▁to', '▁20%', '▁($', '21', 'm', '▁in', '▁US', '▁export', 's', '▁last', '▁year', '),', '▁while', '▁duties', '▁on', '▁fish', '▁hydro', 'ly', 's', 'ate', '▁for', '▁aquatic', '▁feed', '▁fell', '▁from', '▁15%', '▁to', '▁', '5%', '▁($', '35', 'm', '▁in', '▁US', '▁export', 's', '▁in', '▁20', '24', ').', '▁India', '▁also', '▁scrap', 'ped', '▁tariff', 's', '▁on', '▁select', '▁waste', '▁and', '▁scrap', '▁items', ',', '▁', 'a', '▁category', '▁where', '▁US', '▁export', 's', '▁', 'a', 'mounted', '▁to', '▁$', '2.5', 'b', 'n', '▁last', '▁year', '.', '▁Top', '▁US', '▁export', 's', '▁to', '▁India', '▁in', '▁20', '23', '▁included', '▁crude', '▁oil', '▁and', '▁petroleum', '▁products', '▁($', '14', 'b', 'n', '),', '▁LNG', ',', '▁coal', ',', '▁medical', '▁devices', ',', '▁scientific', '▁instruments', ',', '▁scrap', '▁metal', 's', ',', '▁turbo', 'jet', 's', ',', '▁computers', '▁and', '▁almond', 's', '.', '▁\"', 'While', '▁Trump', '▁has', '▁critic', 'ised', '▁India', \"'\", 's', '▁tariff', '▁policies', ',', '▁the', '▁latest', '▁reduction', 's', '▁signal', '▁', 'a', '▁policy', '▁shift', '▁that', '▁could', '▁enhance', '▁US', '▁export', 's', '▁across', '▁various', '▁sectors', ',\"', '▁says', '▁Mr', '▁Sri', 'vas', 't', 'ava', '.', '▁\"', 'With', '▁key', '▁tariff', '▁cuts', '▁on', '▁technology', ',', '▁automobile', 's', ',', '▁industrial', '▁and', '▁waste', '▁import', 's', ',', '▁India', '▁appears', '▁to', '▁be', '▁taking', '▁steps', '▁towards', '▁', 'facilitating', '▁trade', '▁even', '▁as', '▁the', '▁global', '▁trade', '▁environment', '▁remains', '▁', 't', 'ense', '.\"', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs to tokens (strings):\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053cf496-30c1-4fe0-bcf8-c587ee411b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done define functions.\n"
     ]
    }
   ],
   "source": [
    "def translate_long_text(text, chunk_size=500, model_name = 'bert-base-multilingual-cased', device = 'cpu', translation_order = 'translate to zh: '):  # Adjust chunk_size as needed\n",
    "    \"\"\"Translates long text to Chinese by chunking.\"\"\"\n",
    "\n",
    "    \n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name) # load model\n",
    "    model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) # load model tokenizer\n",
    "\n",
    "    src_text = translation_order + text\n",
    "    print(\"src_text: \", src_text)\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\").input_ids\n",
    "    print(\"input ids: \", input_ids)\n",
    "    max_len = tokenizer.model_max_length - 2 # Account for start/end tokens with the tokenizer's max length\n",
    "\n",
    "    translated_chunks = []\n",
    "    for i in range(0, input_ids.shape[1], max_len):\n",
    "        print(\"Working on chunk \", (i // max_len) + 1)\n",
    "        chunk_input_ids = input_ids[:, i:min(i + max_len, input_ids.shape[1])]\n",
    "        generated_tokens = model.generate(chunk_input_ids.to(device))\n",
    "\n",
    "        translated_chunk = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        translated_chunks.append(translated_chunk)\n",
    "\n",
    "    return \"\".join(translated_chunks)  # Combine translated chunks\n",
    "\n",
    "\n",
    "print(\"Done define functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6254b354-3d4f-4d3f-a4a0-c04d8bf5f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.final_layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'lm_head.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_text:  translate to zh: \n",
      "Last week India further slashed import duties on motorcycles, cutting tariffs on heavyweight bikes with engines above 1,600cc from 50% to 30% and smaller ones from 50% to 40%.\n",
      "\n",
      "A pre-emptive move designed to further smoothen the entry of Harley Davidsons into India - and, Delhi hopes, ward off any threat of tariffs. US motorcycle exports to India were worth $3m last year.\n",
      "\n",
      "Donald Trump has marked his return to the White House by brandishing trade measures against America's neighbours and allies as well as its big rival China.\n",
      "\n",
      "India hopes it is ahead of the game - but will its tariff cuts satisfy Trump, or is trade action still on the table?\n",
      "\n",
      "\"Canada and Mexico are literally two arms of the US. If he has acted against them, he could easily act against India too,\" says Ajay Srivastava, founder of the Delhi-based think tank Global Trade Research Institute (GTRI).\n",
      "\n",
      "In their phone conversation late last month, the US president pressed Prime Minister Narendra Modi to buy more US arms and for there to be a fairer trade balance, keeping the pressure on.\n",
      "\n",
      "And during his first term, Trump fixated on India's steep tariffs. He repeatedly slammed the then 100% duty on Harleys as \"unacceptable\", making it a rallying point in his crusade against what he saw as unfair trade practices.\n",
      "\n",
      "In the past he repeatedly branded India a \"tariff king\" and a \"big abuser\" of trade ties.\n",
      "\n",
      "India enjoys a trade surplus with the US, its top trading partner. Bilateral trade crossed $190bn (£150bn) in 2023. Merchandise exports to US have surged 40% to $123bn since 2018, while services trade grew 22% to reach $66bn. Meanwhile, US exports to India stood at $70bn.\n",
      "\n",
      "But beyond bikes, India has zeroed out import taxes on satellite ground installations, benefiting US exporters who supplied $92m worth in 2023.\n",
      "\n",
      "Tariffs on synthetic flavouring essences dropped from 100% to 20% ($21m in US exports last year), while duties on fish hydrolysate for aquatic feed fell from 15% to 5% ($35m in US exports in 2024). India also scrapped tariffs on select waste and scrap items, a category where US exports amounted to $2.5bn last year.\n",
      "\n",
      "Top US exports to India in 2023 included crude oil and petroleum products ($14bn), LNG, coal, medical devices, scientific instruments, scrap metals, turbojets, computers and almonds.\n",
      "\n",
      "\"While Trump has criticised India's tariff policies, the latest reductions signal a policy shift that could enhance US exports across various sectors,\" says Mr Srivastava.\n",
      "\n",
      "\"With key tariff cuts on technology, automobiles, industrial and waste imports, India appears to be taking steps towards facilitating trade even as the global trade environment remains tense.\"\n",
      "\n",
      "input ids:  tensor([[   101,  37241,  23953,  10114,    194,  10237,    131,  14812,  16118,\n",
      "          11098,  14586,  38523,  73491,  10336,  67622,  40266,  10135, 101923,\n",
      "          10107,    117,  63039,  24948,  31025,  10107,  10135,  18296,  31869,\n",
      "          99345,  10107,  10169,  34073,  16038,    122,    117,  11796,  28044,\n",
      "          10188,  10462,    110,  10114,  10244,    110,  10111,  23309,  35688,\n",
      "          10188,  10462,    110,  10114,  10533,    110,    119,    138,  12229,\n",
      "            118,  10266, 100318,  18577,  15513,  10114,  14586,  67601,  10136,\n",
      "          10105,  14722,  10108,  65721,  41130,  10107,  10708,  11098,    118,\n",
      "          10111,    117,  18720,  82467,    117,  60026,  11898,  11178,  41256,\n",
      "          10108,  24948,  31025,  10107,    119,  10808, 101923,  59220,  10107,\n",
      "          10114,  11098,  10309,  43509,    109,    124,  10147,  12469,  10924,\n",
      "            119,  16437,  29846,  10393,  26981,  10226,  15079,  10114,  10105,\n",
      "          12136,  11545,  10155,  23467, 103760,  18908,  38606,  11327,  11440,\n",
      "            112,    187,  11888,  15774,  97497,  10111,  67675,  10146,  11206,\n",
      "          10146,  10474,  22185,  24792,  11593,    119,  11098,  82467,  10271,\n",
      "          10124,  35629,  10108,  10105,  11661,    118,  10473,  11337,  10474,\n",
      "          24948,  31025,  73311,  20694,  10291,  24203,  29846,    117,  10345,\n",
      "          10124,  18908,  14204,  12647,  10135,  10105,  21783,    136,    107,\n",
      "          11635,  10111,  10490,  10301,  59232,  10551,  28150,  10108,  10105,\n",
      "          10808,    119,  14535,  10261,  10393,  48809,  11327,  11345,    117,\n",
      "          10261,  12174,  35024,  19833,  11327,  11098,  16683,    117,    107,\n",
      "          22153,    138,  78761,  16098,  14139,  29102,    117,  22826,  10108,\n",
      "          10105,  18720,    118,  11610,  27874,  28671,  13037,  15006,  12959,\n",
      "          12031,    113,  27987,  46876,    114,    119,  10167,  10455,  41008,\n",
      "          72028,  13002,  12469,  14064,    117,  10105,  10808,  12931,  33834,\n",
      "          10162,  19924,  14355,  10685,  68712,  10288,  78616,  10116,  10114,\n",
      "          47715,  10798,  10808,  28150,  10111,  10142,  11155,  10114,  10347,\n",
      "            169,  14131,  10129,  18908,  40162,    117,  51318,  10105,  23460,\n",
      "          10135,    119,  12689,  10939,  10226,  10422,  13719,    117,  29846,\n",
      "          14045,  14220,  11912,  10135,  11098,    112,    187, 102718,  24948,\n",
      "          31025,  10107,    119,  10357,  77998,  38523,  11008,  21986,  10105,\n",
      "          11059,  10407,    110,  34238,  10135,  65721,  10107,  10146,    107,\n",
      "          10153,  48798,  23122,  11203,    107,    117,  14293,  10271,    169,\n",
      "          58575,  10230,  12331,  10106,  10226,    171,  94117,  11327,  12976,\n",
      "          10261,  17112,  10146,  10119,  82198,  18908,  41011,    119,  10167,\n",
      "          10105,  17781,  10261,  77998,  23467,  10336,  11098,    169,    107,\n",
      "          24948,  31025,  20636,    107,  10111,    169,    107,  22185,  51401,\n",
      "          10129,    107,  10108,  18908,  45169,    119,  11098,  84874,  10107,\n",
      "            169,  18908,  10326,  70413,  10169,  10105,  10808,    117,  10474,\n",
      "          12364,  46663,  22825,    119,  72755,  98161,  18908,  56433,    109,\n",
      "          16664,  71136,    113, 110642,  28847,  71136,    114,  10106,  22171,\n",
      "          10884,    119,  10734,  47019, 103845,  59220,  10107,  10114,  10808,\n",
      "          10529,  69824,  10162,  10533,    110,  10114,    109,  16968,  71136,\n",
      "          11764,  10434,    117,  11371,  12639,  18908,  23616,  10306,    110,\n",
      "          10114,  24278,    109,  12215,  71136,    119,  43309,    117,  10808,\n",
      "          59220,  10107,  10114,  11098,  39320,  10160,    109,  10923,  71136,\n",
      "            119,  16976,  28569,  99345,  10107,    117,  11098,  10393,  28375,\n",
      "          10336,  10950,  67622,  49870,  10135,  31227,  16912,  60410,    117,\n",
      "          43074,  10230,  10808,  59220,  10901,  10479,  65194,    109,  12458,\n",
      "          10147,  43509,  10106,  22171,  10884,    119,  14248,  40738,  10107,\n",
      "          10135,  14379,  64899,  58768,  99749,  10230,  78038,  10107,  30241,\n",
      "          10188,  10407,    110,  10114,  10197,    110,    113,    109,  10296,\n",
      "          10147,  10106,  10808,  59220,  10107,  12469,  10924,    114,    117,\n",
      "          11371,  40266,  10135,  26228,  56888,  22698,  12682,  12436,  10142,\n",
      "          78340,  44988,  25194,  10188,  10208,    110,  10114,    126,    110,\n",
      "            113,    109,  10803,  10147,  10106,  10808,  59220,  10107,  10106,\n",
      "          22171,  11011,    114,    119,  11098,  10379,    187,  40333,  44357,\n",
      "          24948,  31025,  10107,  10135,  47054,  59158,  10111,    187,  40333,\n",
      "          10410,  34109,    117,    169,  29737,  10940,  10808,  59220,  10107,\n",
      "          24074,  10336,  10114,    109,    123,    119,    126,  71136,  12469,\n",
      "          10924,    119,  12685,  10808,  59220,  10107,  10114,  11098,  10106,\n",
      "          22171,  10884,  12742,    171,  97417,  21073,  10111,  99435,  36581,\n",
      "          20895,    113,    109,  10247,  71136,    114,    117,    149,  34065,\n",
      "            117,  39145,    117,  19436,  38120,    117,  23301,  26096,    117,\n",
      "            187,  40333,  10410,  13795,  10107,    117,  89833,  18533,  10107,\n",
      "            117,  58838,  10111,  10164,  97369,    119,    107,  14600,  29846,\n",
      "          10393,  91588,  11098,    112,    187,  24948,  31025,  38572,    117,\n",
      "          10105,  50908,  51608,  10107,  25495,    169,  14368,  51467,  10189,\n",
      "          12174,  10110, 100586,  10808,  59220,  10107,  15130,  13547,  69714,\n",
      "            117,    107,  22153,  12916,  16098,  14139,  29102,    119,    107,\n",
      "          12613,  18444,  24948,  31025,  73311,  10135,  19765,    117,  35599,\n",
      "          10107,    117,  18138,  10111,  59158,  67622,  10107,    117,  11098,\n",
      "          20296,  10114,  10347,  18084,  50879,  18095, 107159,  85655,  10230,\n",
      "          18908,  13246,  10146,  10105,  18331,  18908,  26069,  19602,  11769,\n",
      "          10341,    119,    107,    102]])\n",
      "Working on chunk  1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_long_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result\n",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m, in \u001b[0;36mtranslate_long_text\u001b[0;34m(text, chunk_size, model_name, device, translation_order)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on chunk \u001b[39m\u001b[38;5;124m\"\u001b[39m, (i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m max_len) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m chunk_input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, i:\u001b[38;5;28mmin\u001b[39m(i \u001b[38;5;241m+\u001b[39m max_len, input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m---> 19\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m translated_chunk \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m translated_chunks\u001b[38;5;241m.\u001b[39mappend(translated_chunk)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/firsttextproj-0PxDljZr/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/firsttextproj-0PxDljZr/lib/python3.10/site-packages/transformers/generation/utils.py:2033\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2030\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2032\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m-> 2033\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[1;32m   2038\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/firsttextproj-0PxDljZr/lib/python3.10/site-packages/transformers/generation/utils.py:1876\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens\u001b[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[0m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;66;03m# Sanity checks/warnings\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m decoder_start_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1878\u001b[0m     )\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():  \u001b[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1881\u001b[0m         eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m isin_mps_friendly(elements\u001b[38;5;241m=\u001b[39meos_token_tensor, test_elements\u001b[38;5;241m=\u001b[39mpad_token_tensor)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m   1883\u001b[0m     ):\n",
      "\u001b[0;31mValueError\u001b[0m: `decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation."
     ]
    }
   ],
   "source": [
    "\n",
    "result = translate_long_text(text_2)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1bdf1c-2021-4954-bd2b-ed3d2207e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-08 20:21:40--  https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.128, 13.35.7.38, 13.35.7.82, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1176355829 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘lid218e.bin.1’\n",
      "\n",
      "lid218e.bin.1       100%[===================>]   1.09G  10.7MB/s    in 1m 43s  \n",
      "\n",
      "2025-02-08 20:23:23 (10.9 MB/s) - ‘lid218e.bin.1’ saved [1176355829/1176355829]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f39f9e4-a537-4bbb-a909-07e0a8e8eae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "\n",
    "\n",
    "lang = detect(text)\n",
    "print(lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e988bada-f3a2-4890-8203-942aa33c33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "source_lang = 'eng_Latn'  #'arb_Arab'\n",
    "target_lang = 'zho_Hans'   #'spa_Latn'\n",
    "\n",
    "translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=source_lang, tgt_lang=target_lang, max_length = 400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b720246-c06d-48fa-a086-dfb0e7308abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我叫劳拉,我喜欢打篮球.\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is Laura. I like to play basketball.\"\n",
    "\n",
    "output = translator(text)\n",
    "translated_text = output[0]['translation_text']\n",
    "print(translated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6814e1d7-075a-4730-84a2-8ea8cd7b0517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zho_Hans'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb918d4e-2fe9-4341-9bc3-453d90ee8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def translate_text(text, checkpoint = 'facebook/nllb-200-distilled-600M', chunk_size=500, src_lang = 'eng_Latn', target_lang = 'zho_Hans'): # Adjust chunk_size as needed\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    max_input_length = tokenizer.model_max_length - 2 # Account for start/end tokens with the tokenizer's max length\n",
    "    \n",
    "    translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=source_lang, tgt_lang=target_lang, max_length = 400)\n",
    "    \n",
    "    translated_text = \"\"\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        chunk_processed = translator(chunk)[0]['translation_text']\n",
    "\n",
    "        translated_text += chunk_processed\n",
    "        \n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf8496d-1a1a-4e97-aedb-36843ed1de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 155 ms, total: 4min 14s\n",
      "Wall time: 42.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'上周,印度进一步削减了对摩托车的进口关税,将超过1600cc的发动机的重量级自行车的关税从50%降至30%,较小的自行车从50%降至40%.这是旨在进一步缓解哈雷·戴维森进入印度的预防措施,德里希望避免任何关税威胁.去年美国对印度的摩托车出口价值为300万美元.唐纳德特朗普通过对美国邻国和盟友以及大竞争对手中国的贸易措施标志着他的回归白宫.印度希望它在比赛前面 - - 但它将关税削减特朗普,还是贸易行动仍在桌上? \"加拿大和墨西哥是美国的两支武器.如果他对抗他们,他很容易对印度采取行动\",德里全球贸易研究院创始人阿贾瓦斯塔说.过去,他曾多次称印度为\"关税王\"和\"大滥用商业关系\".印度与其最大贸易伙伴美国享有贸易顺差.2023年,双边贸易额突破了1.9亿美元 (150亿英).自2018年以来,美国的商品出口增长了40%至123亿美元,而服务贸易增长了22%,达到66亿美元.此同时,美国对印度的出口价值达70亿美元.但除了摩托车之外,印度已在卫星上提供零地供应,使美国对水力垃圾出口额从2023年下降到2023年,美国对水力垃圾出口额也下降了15%.美国对印度的出口量最高为25亿美元.2023年,美国对印度的最大出口量包括原油和石油产品 (14亿美元),液化天然气,煤炭,医疗器械,科学仪器,废金属,轮机,计算机和杏仁. \"尽管特朗普批评了印度的关税政策,但最近的削减表明了政策转变,这可能会提高美国在各个部门的出口量\",斯里瓦斯塔瓦先生说.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "res = translate_chunks(text_2, chunk_size = max_input_length)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f83dc-c274-44d7-b77c-2cd910358148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first_text_env",
   "language": "python",
   "name": "firsttext-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
